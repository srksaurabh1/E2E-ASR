{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world3f2\n"
     ]
    }
   ],
   "source": [
    "print('hello world3f2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-04-01 17:04:55 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "#### Import Dependencies ####\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.utils import exp_manager\n",
    "import pytorch_lightning\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from nemo.core import adapter_mixins\n",
    "from pytorch_lightning import Trainer\n",
    "from nemo.collections.common.parts.adapter_modules import LinearAdapterConfig\n",
    "import torch\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrksaurbzz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "### API KEY : 185835c122601380ffad33b46fc36065559c4476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-04-01 17:05:01 mixins:170] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-04-01 17:05:01 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /code/03_manifest/ulca-train-v3.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 16.7\n",
      "    min_duration: 0.1\n",
      "    \n",
      "[NeMo W 2023-04-01 17:05:01 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /code/manifest/ulca-eval.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2023-04-01 17:05:01 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: /code/manifest/ulca-eval.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-04-01 17:05:01 features:287] PADDING: 0\n",
      "[NeMo I 2023-04-01 17:05:08 save_restore_connector:247] Model EncDecCTCModelBPE was successfully restored from /home/saurabh/pretrained_models/Conformer_hi/stt_hi_conformer_ctc_medium.nemo.\n",
      "Updated encoder _target_ model:  nemo.collections.asr.modules.conformer_encoder.ConformerEncoderAdapter\n",
      "[NeMo I 2023-04-01 17:05:09 mixins:170] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-04-01 17:05:09 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /code/03_manifest/ulca-train-v3.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 16.7\n",
      "    min_duration: 0.1\n",
      "    \n",
      "[NeMo W 2023-04-01 17:05:09 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /code/manifest/ulca-eval.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2023-04-01 17:05:09 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: /code/manifest/ulca-eval.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-04-01 17:05:09 features:287] PADDING: 0\n",
      "[NeMo I 2023-04-01 17:05:10 save_restore_connector:247] Model EncDecCTCModelBPE was successfully restored from /home/saurabh/pretrained_models/Conformer_hi/stt_hi_conformer_ctc_medium.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-04-01 17:05:11 collections:193] Dataset loaded with 12657 files totalling 17.58 hours\n",
      "[NeMo I 2023-04-01 17:05:11 collections:194] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2023-04-01 17:05:12 collections:193] Dataset loaded with 3207 files totalling 4.45 hours\n",
      "[NeMo I 2023-04-01 17:05:12 collections:194] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2023-04-01 17:05:12 collections:193] Dataset loaded with 3207 files totalling 4.45 hours\n",
      "[NeMo I 2023-04-01 17:05:12 collections:194] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:712] Setting adapter 'Confomer_telegu' status : Enabled = False\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:727] Setting adapter 'Confomer_telegu' status : Enabled = True\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.0.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.1.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.2.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.3.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.4.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.5.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.6.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.7.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.8.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.9.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.10.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.11.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.12.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.13.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.14.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.15.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.16.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:398] Froze module encoder.layers.17.conv.batch_norm: BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "[NeMo I 2023-04-01 17:05:12 adapter_mixins:428] Unfrozen adapter : Confomer_telegu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-04-01 17:05:12 exp_manager:710] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-04-01 17:05:12 exp_manager:593] Resuming from /home/saurabh/experiments/Telegu_IS_64/checkpoints/teleguISTrain_scratch--val_wer=0.6287-epoch=300-last.ckpt\n",
      "[NeMo I 2023-04-01 17:05:12 exp_manager:373] Experiments will be logged at /home/saurabh/experiments/Telegu_IS_64\n",
      "[NeMo I 2023-04-01 17:05:12 exp_manager:791] TensorboardLogger has been set up\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109075b38d6a4a4c87022090a1eaf076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01667762024999888, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/saurabh/experiments/wandb/run-20230401_170512-6d4kdhdh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srksaurbzz/Telegu_IS_64/runs/6d4kdhdh' target=\"_blank\">run01</a></strong> to <a href='https://wandb.ai/srksaurbzz/Telegu_IS_64' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srksaurbzz/Telegu_IS_64' target=\"_blank\">https://wandb.ai/srksaurbzz/Telegu_IS_64</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srksaurbzz/Telegu_IS_64/runs/6d4kdhdh' target=\"_blank\">https://wandb.ai/srksaurbzz/Telegu_IS_64/runs/6d4kdhdh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-04-01 17:05:14 exp_manager:806] WandBLogger has been set up\n"
     ]
    }
   ],
   "source": [
    "####cONSTANTS #####\n",
    "TRAIN_MANIFEST = '/home/saurabh/manifest/train.json'\n",
    "VALID_MANIFEST = '/home/saurabh/manifest/valid.json'\n",
    "\n",
    "# Model save path\n",
    "model_path_pretrained = '/home/saurabh/pretrained_models/Conformer_hi/stt_hi_conformer_ctc_medium.nemo'\n",
    "model_32 = '/home/saurabh/experiments/Telegu_IS_32/checkpoints/teleguISTrain_scratch_32.nemo'\n",
    "model_64 = '/home/saurabh/experiments/Telegu_IS_64/checkpoints/teleguISTrain_scratch.nemo'\n",
    "\n",
    "model_path = model_path_pretrained\n",
    "\n",
    "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "# accelerator = 'cpu'\n",
    "max_steps = -1\n",
    "max_epochs = 10\n",
    "\n",
    "#Spectogram augmentation\n",
    "lr = 5\n",
    "weight_decay = 0\n",
    "warmup_steps = 10000\n",
    "\n",
    "\n",
    "## Adapter \n",
    "adapter_name = 'Confomer_telegu'\n",
    "adapter_dim = 64\n",
    "adapter_activtion = 'swish'\n",
    "adapter_norm_position = 'pre'\n",
    "\n",
    "\n",
    "#EXP\n",
    "a64 = 'Telegu_IS_64'\n",
    "a32 = 'Telegu_IS_32' \n",
    "a128 = 'Telegu_IS_128' \n",
    "\n",
    "exp_direc=f'/home/saurabh/experiments' # Don't change\n",
    "name_exp = a64 # where it will be saved\n",
    "#wandb_karwads\n",
    "name_wandb='run01'\n",
    "project_wandb = a64\n",
    "\n",
    "\n",
    "\n",
    "#### Load model #####\n",
    "model= nemo_asr.models.ASRModel.restore_from(model_path)\n",
    "\n",
    "\n",
    "##### adpter metadata #####\n",
    "def update_model_config_to_support_adapter(model_cfg):\n",
    "  with open_dict(model_cfg):\n",
    "    adapter_metadata = adapter_mixins.get_registered_adapter(model_cfg.encoder._target_)\n",
    "    if adapter_metadata is not None:\n",
    "      model_cfg.encoder._target_ = adapter_metadata.adapter_class_path\n",
    "  \n",
    "  print(\"Updated encoder _target_ model: \", model_cfg.encoder._target_)\n",
    "  return model_cfg\n",
    "\n",
    "cfg = update_model_config_to_support_adapter(model.cfg)\n",
    "model = nemo_asr.models.ASRModel.restore_from(model_path, override_config_path=cfg)\n",
    "\n",
    "\n",
    "##### Trainer #####\n",
    "trainer = Trainer(devices=1, accelerator=accelerator, max_steps=max_steps,\n",
    "                  enable_checkpointing=False, logger=False,\n",
    "                  log_every_n_steps=5, check_val_every_n_epoch=3, \n",
    "                  max_epochs=max_epochs)\n",
    "\n",
    "model.set_trainer(trainer)\n",
    "\n",
    "\n",
    "##### Dataloader for updater #######\n",
    "with open_dict(model.cfg):\n",
    "  model.cfg.train_ds.manifest_filepath = TRAIN_MANIFEST\n",
    "  model.cfg.train_ds.is_tarred = False\n",
    "  # model.cfg.train_ds.tarred_audio_filepaths = None \n",
    "  model.cfg.train_ds.num_workers = 2\n",
    "  model.cfg.train_ds.batch_size = 16\n",
    "\n",
    "  model.cfg.validation_ds.manifest_filepath = VALID_MANIFEST\n",
    "  model.cfg.validation_ds.num_workers = 2\n",
    "  model.cfg.validation_ds.batch_size = 16\n",
    "\n",
    "model.setup_training_data(model.cfg.train_ds)\n",
    "model.setup_multiple_validation_data(model.cfg.validation_ds)\n",
    "model.setup_multiple_test_data(model.cfg.validation_ds)\n",
    "\n",
    "\n",
    "adapter_cfg = LinearAdapterConfig(\n",
    "    in_features=model.cfg.encoder.d_model,\n",
    "    dim=adapter_dim,\n",
    "    activation=adapter_activtion,\n",
    "    norm_position=adapter_norm_position,\n",
    ")\n",
    "\n",
    "model.add_adapter(name=adapter_name, cfg=adapter_cfg)\n",
    "model.set_enabled_adapters(enabled=False) # disable all adapters\n",
    "model.set_enabled_adapters(name=adapter_name, enabled=True)\n",
    "model.freeze()\n",
    "model.unfreeze_enabled_adapters()\n",
    "\n",
    "\n",
    "##### Experiment Manager #######\n",
    "exp_config = exp_manager.ExpManagerConfig(\n",
    "    exp_dir=exp_direc,\n",
    "    name=name_exp,\n",
    "    \n",
    "    checkpoint_callback_params=exp_manager.CallbackParams(\n",
    "        monitor='val_wer',\n",
    "        mode='min',\n",
    "        always_save_nemo=True,\n",
    "        save_best_model=True,\n",
    "    ),\n",
    "    \n",
    "    resume_if_exists=True,\n",
    "    resume_ignore_no_checkpoint=True,\n",
    "    create_tensorboard_logger= True,\n",
    "    create_checkpoint_callback= True,\n",
    "  \n",
    "    create_wandb_logger=True,\n",
    "    wandb_logger_kwargs={\n",
    "        'name':name_wandb, \n",
    "        'project':project_wandb\n",
    "    },\n",
    ")\n",
    "exp_config=OmegaConf.structured(exp_config)\n",
    "logdir = exp_manager.exp_manager(trainer, exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name              | Type                              | Params\n",
       "------------------------------------------------------------------------\n",
       "0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0     \n",
       "1 | encoder           | ConformerEncoderAdapter           | 31.1 M\n",
       "2 | decoder           | ConvASRDecoder                    | 33.2 K\n",
       "3 | loss              | CTCLoss                           | 0     \n",
       "4 | spec_augmentation | SpectrogramAugmentation           | 0     \n",
       "5 | _wer              | WERBPE                            | 0     \n",
       "------------------------------------------------------------------------\n",
       "599 K     Trainable params\n",
       "30.5 M    Non-trainable params\n",
       "31.1 M    Total params\n",
       "124.551   Total estimated model params size (MB)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = '/home/saurabh/saved_model/Telugu_IS128.nemo'\n",
    "model.save_to(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "nemo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
